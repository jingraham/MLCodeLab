{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar_sample_solution.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jingraham/MLCodeLab/blob/master/lab2/cifar_sample_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aE0EfZTNUEd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load PyTorch and check if GPUs are available via NVIDIA CUDA\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkTbfbSvNUEp",
        "colab_type": "text"
      },
      "source": [
        "# The Task: CIFAR Classification\n",
        "<img src=\"https://raw.githubusercontent.com/yala/MLCodeLab/master/lab2/cifar.png\">\n",
        "\n",
        "In this lab, we'll build a neural network to classify images on CIFAR 10.\n",
        "\n",
        "\n",
        "\n",
        "## Step 1: Loading Data and Preprocessing\n",
        "Let's start by loading the data.\n",
        "We're going to normalize our images to have 0 mean, and unit variance. We'll do this using some torchvision transforms. This generally helps stablize learning, and is common practice. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikae_1ttNUEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalize_image = transforms.Compose([\n",
        "                           transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))   \n",
        "                       ])\n",
        "\n",
        "all_train = datasets.CIFAR10('data', train=True, download=True,\n",
        "                        transform=normalize_image)\n",
        "num_train = int(len(all_train)*.8)\n",
        "train = [all_train[i] for i in range(num_train)]\n",
        "dev = [all_train[i] for i in range(num_train,len(all_train))]\n",
        "test = datasets.CIFAR10('data', train=False, download=True, \n",
        "                      transform=normalize_image)\n",
        "                           \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCPmNnUQNUE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train[0][0].size()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZsXx8peNUE-",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Building a model\n",
        "\n",
        "All pytorch models should be implemented as instances of `nn.Module`. \n",
        "\n",
        "To build a model you need to:\n",
        "a) define what parameters it'll need in it's `__init__` function\n",
        "b) define the model's computation, using those parameters, in a forward function.\n",
        "\n",
        "\n",
        "Experiment with different neural architectures. I recommend starting with a simple linear model and building up from there. Here, I implement a simple 4-layer convolutional neural network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmsRjZG5NUE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 128, kernel_size=10)\n",
        "        self.conv2 = nn.Conv2d(128, 256, kernel_size=3)\n",
        "        self.conv3 = nn.Conv2d(256, 512, kernel_size=3)\n",
        "        self.fc = nn.Linear(512, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        batch_size, num_channels, height, width = x.size()\n",
        "        hidden = F.relu(self.conv1(x))\n",
        "        hidden = F.relu(self.conv2(hidden))\n",
        "        hidden = F.relu(self.conv3(hidden))\n",
        "        hidden = hidden.view((batch_size, 512, -1))\n",
        "        hidden = torch.mean(hidden, dim=-1)\n",
        "        logit = self.fc(hidden)\n",
        "        return logit\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WHb4bI5NUFD",
        "colab_type": "text"
      },
      "source": [
        "## Step 3. Defining our training procedure\n",
        "\n",
        "Set your batch size, learning rate, number of epochs etc. Experiment with various hyper parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0bg1sWwNUFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training settings\n",
        "batch_size =  32\n",
        "epochs = 25\n",
        "lr = 1e-2\n",
        "momentum=.9\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "dev_loader = torch.utils.data.DataLoader(dev, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "model = Model()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXZBov0yNUFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def train_epoch( model, train_loader, optimizer, epoch):\n",
        "    model.train() # Set the nn.Module to train mode. \n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    num_samples = len(train_loader.dataset)\n",
        "    for batch_idx, (x, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        total_loss += loss.detach() # Don't keep computation graph \n",
        "\n",
        "    print('Train Epoch: {} \\tLoss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "            epoch, total_loss / num_samples, \n",
        "            correct, \n",
        "            num_samples,\n",
        "            100. * correct / num_samples))\n",
        "\n",
        "def eval_epoch(model, test_loader, name):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for x, target in test_loader:\n",
        "        output = model(x)\n",
        "        test_loss += F.cross_entropy(output, target).item() # sum up batch loss\n",
        "        pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\n{} set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        name,\n",
        "        test_loss, \n",
        "        correct, \n",
        "        len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV7RxuTgNUFQ",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oS_E3DLNUFT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_epoch(model, train_loader, optimizer, epoch)\n",
        "    eval_epoch(model,  dev_loader, \"Dev\")\n",
        "    print(\"---\")\n",
        "\n",
        "# Performance:\n",
        "#Train Epoch: 25         Loss: 0.0130, Accuracy: 34145/40000 (85%)\n",
        "#Dev set: Average loss: 0.0263, Accuracy: 7349/10000 (73%)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvnVDQjVNUFa",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnRCLORuNUFc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbp4J05oNUFj",
        "colab_type": "text"
      },
      "source": [
        "## Run your best model on Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05H22miJNUFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_epoch(model,  test_loader, \"Test\")\n",
        "# Performance from last run: \n",
        "#Test set: Average loss: 0.0285, Accuracy: 7239/10000 (72%)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}